{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook for simplicial and relational agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bbY5EoOwhLwb",
    "outputId": "265c2959-c981-42d3-e0cb-b54d1ed9b180"
   },
   "source": [
    "This is the training notebook for the simplicial agent of the paper \"Logic and the 2-Simplicial Transformer\", by James Clift, Dmitry Doryn, Daniel Murfet and James Wallbridge. The official repository is here: https://github.com/dmurfet/2simplicialtransformer.\n",
    "\n",
    "Libraries used:\n",
    "* Keras, TensorFlow (1.13.1)\n",
    "* Keras-Transformer (https://github.com/kpot/keras-transformer)\n",
    "* RLlib / Ray\n",
    "* OpenAI Gym\n",
    "\n",
    "Throughout the file are various quotes, always from the paper \n",
    "\n",
    "- V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li, I. Babuschkin, K. Tuyls, D. Reichert, T. Lillicrap, E. Lockhart, M. Shanahan, V. Langston, R. Pascanu, M. Botvinick, O. Vinyals and P. Battaglia, [Deep reinforcement learning with relational inductive biases](https://openreview.net/forum?id=HkxaFoC9KQ), in Proceedings of the International Conference on Learning Representations (ICLR), 2019.\n",
    "\n",
    "## Installation\n",
    "\n",
    "There is a more detailed installation guide on the GitHub repoistory. Note that to be safe, you should have the 0.7.0.dev2 version of ray\n",
    "\n",
    "```\n",
    "pip3 install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.7.0.dev2-cp36-cp36m-manylinux1_x86_64.whl\n",
    "```\n",
    "\n",
    "and have patched the Ray RLlib files using the versions distributed with this notebook, for instance:\n",
    "\n",
    "```\n",
    "cp ~/2simplicialtransformer/python/policy_evaluator-0.7.0.dev2-edited.py ~/.local/lib/python3.6/site-packages/ray/rllib/evaluation/policy_evaluator.py\n",
    "cp ~/2simplicialtransformer/python/impala-0.7.0.dev2-edited.py ~/.local/lib/python3.6/site-packages/ray/rllib/agents/impala/impala.py\n",
    "```\n",
    "\n",
    "This of course will depend on where you have Python installed. You will need the libraries for the environment and agents to be in your Python path:\n",
    "\n",
    "```\n",
    "ln -s ~/2simplicialtransformer/env/bridge_boxworld.py ~/.local/lib/python3.6/site-packages/bridge_boxworld.py\n",
    "ln -s ~/2simplicialtransformer/agent/agent_relational.py ~/.local/lib/python3.6/site-packages/agent_relational.py\n",
    "ln -s ~/2simplicialtransformer/agent/agent_simplicial.py ~/.local/lib/python3.6/site-packages/agent_simplicial.py\n",
    "```\n",
    "\n",
    "## Before running this notebook\n",
    "\n",
    "You will need to have Ray running, e.g. with `ray start --head --redis-port=6379 --num-cpus=64 --num-gpus=1` and then you need to copy the output of ray start into the `REDIS_ADDRESS` flag below. Also set the number of virtual CPUs and GPUs. Ray will automatically write Tensorboard files, `tensorboard --logdir=~/ray_results > /dev/null 2>&1 &`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_ADDRESS=\"192.168.2.2:6379\" # output of ray start\n",
    "AGENT = \"base_model\" # simplicial_model (simplicial agent), base_model (relational agent)\n",
    "LOG_FILENAME = \"/home/murfetd/log-base-test.txt\" # should vary between runs\n",
    "NUM_WORKERS = 63 # one less than your number of vCPUs\n",
    "NUM_GPUS = 1\n",
    "TIMESTEPS_TOTAL = 100e8 # training will never finish, interrupt to stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fngCiF8MhVPu"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import base64\n",
    "from random import randint, sample, shuffle\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # turn off LOG and WARNINGs \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import logging\n",
    "logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import grid_search\n",
    "\n",
    "import gym\n",
    "gym.logger.set_level(40) #error only\n",
    "#gym.logger.set_level(10) #debug\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from gym.spaces import Discrete, Box\n",
    "import copy\n",
    "import colorsys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "The environment is an object conforming to the OpenAI Gym Environment API. The environment is split between a part in the notebook (below) and a part in the bridge BoxWorld environment library. There is no good reason for this, but nobody promised you a notebook conforming to the highest standards of software engineering. It works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcOUtHQ4hYPI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bridge_boxworld as boxworld\n",
    "\n",
    "def createColoursArrayHSV():\n",
    "  colours = []\n",
    "  \n",
    "  for h in range(0, 360, 18):\n",
    "    rgb = colorsys.hsv_to_rgb(h/360, 0.7, 0.8)\n",
    "    colours.append(tuple([int(round(c*255.0)) for c in rgb]))\n",
    "    \n",
    "  # randomise the order of the colours\n",
    "  random.shuffle(colours)\n",
    "  \n",
    "  colours.insert(0,(255,255,255))\n",
    "\n",
    "  return colours\n",
    "\n",
    "def board_to_rgb(b,inv,colours):\n",
    "  \"\"\"Converts a Clift board into an RGB board\"\"\"\n",
    "  \n",
    "  # Keep in mind that in boxworld.py, b[x][y] is the entry in \n",
    "  # x-coordinate x and y-coordinate y with the origin in the\n",
    "  # upper left corner of the screen, but in rgb_board (according\n",
    "  # to matplotlib convention) [y,x,:] is the RGB vector for the\n",
    "  # tile in position (x,y).\n",
    "  \n",
    "  numCols = len(b)\n",
    "  numRows = len(b[0])\n",
    "\n",
    "  # We have an extra column for the inventory\n",
    "  # \"Keys that an agent has in its possession are depicted in the input\n",
    "  # observation as a pixel in the top-left corner.\"\"\n",
    "  rgb_board = np.zeros([numRows,numCols+1,3],dtype=np.uint8)\n",
    "  \n",
    "  for x in range(numCols+1):\n",
    "    for y in range(numRows):\n",
    "        if( x < numCols ):\n",
    "            c = b[x][y]\n",
    "        else: # inventory\n",
    "            if( y < len(inv) ):\n",
    "                c = [inv[y]]\n",
    "            else:\n",
    "                c = []\n",
    "                    \n",
    "        if( len(c) == 0 and x == numCols ):\n",
    "            rgb_board[y,x,:] = [10,10,10]\n",
    "        elif( len(c) == 0 ):\n",
    "            rgb_board[y,x,:] = [199,199,199]\n",
    "        elif( len(c) == 1 ):\n",
    "            rgb_board[y,x,:] = colours[c[0]]\n",
    "\n",
    "  return rgb_board\n",
    "\n",
    "# NOTE: we have borrowed from this example\n",
    "# https://github.com/ray-project/ray/blob/master/python/ray/rllib/examples/custom_env.py\n",
    "\n",
    "class BoxWorld(gym.Env):\n",
    "\n",
    "    # The state of the game consists of\n",
    "    #\n",
    "    # 1. The board, an array of integers in [0,sol_length] of shape (num_rows,num_cols)\n",
    "    # 2. The player position, a pair (y,x) of integers\n",
    "    # 3. The inventory, a list of nonzero integers\n",
    "    #\n",
    "    # When asked for an observation of the environment, we return rgb_board\n",
    "    # applied to the current board, with the player position painted PLAYER_COLOUR\n",
    "    # Note that observations are generated with RGB range 0-255. There is an extra\n",
    "    # column added on the right hand side, which contains (from the top) a rendering\n",
    "    # of the content of the player inventory, one square per key (if this exceeds\n",
    "    # the size of the screen, too bad! TODO).\n",
    "    \n",
    "    metadata = {\n",
    "        'render.modes': ['rgb_array'],\n",
    "        'video.frames_per_second': 30\n",
    "    }\n",
    " \n",
    "    reward_range = (-float('inf'), float('inf'))\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.num_rows         = config[\"num_rows\"]\n",
    "        self.num_cols         = config[\"num_cols\"]\n",
    "        self.max_decoy_paths  = config[\"max_decoy_paths\"]\n",
    "        self.max_decoy_length = config[\"max_decoy_length\"]\n",
    "        self.min_sol_length   = config[\"min_sol_length\"]\n",
    "        self.max_sol_length   = config[\"max_sol_length\"]\n",
    "        self.episode_horizon  = config[\"episode_horizon\"]\n",
    "        self.monitor_interval = config[\"monitor_interval\"]\n",
    "        self.multi_lock       = config[\"multi_lock\"]\n",
    "        self.has_bridge       = config[\"has_bridge\"]\n",
    "        self.num_steps        = 0\n",
    "        \n",
    "        # The valid entries in a square are 0,....,sol_length\n",
    "        \n",
    "        # For spaces see https://github.com/openai/gym/blob/master/gym/spaces/box.py\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0,high=255, shape=(self.num_rows,self.num_cols+1,3), dtype=np.uint8)\n",
    "\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "        \n",
    "        self.num_steps += 1\n",
    "        \n",
    "        # actions\n",
    "        # 0 = left\n",
    "        # 1 = up\n",
    "        # 2 = right\n",
    "        # 3 = down\n",
    "        \n",
    "        if( action == 0 ): frame_input = [-1,0]\n",
    "        if( action == 1 ): frame_input = [0,-1]\n",
    "        if( action == 2 ): frame_input = [1,0]\n",
    "        if( action == 3 ): frame_input = [0,1]\n",
    "          \n",
    "        old_inventory = copy.deepcopy(self.inventory)\n",
    "        \n",
    "        self.board, self.pos, self.inventory = boxworld.updateState(self.board,self.pos,self.inventory,frame_input,self.multi_lock)\n",
    "        \n",
    "        # p.14 of Zambaldi et al states:\n",
    "        # \"An agent receives a reward of +10 for collecting the gem, +1 for opening a box\n",
    "        # in the solution path and −1 for opening a distractor box. A level terminates\n",
    "        # immediately after the gem is collected or a distractor box is opened.\"\n",
    "        #\n",
    "        # Currently we give +10 for the gem, and +1 for a change in inventory, which\n",
    "        # is a proxy for opening a box\n",
    "        \n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        \n",
    "        # If you get the Gem, game over (you win)\n",
    "        if( boxworld.GOAL_TILE in self.inventory ):\n",
    "          done = True\n",
    "          reward += 10.0\n",
    "\n",
    "        # If you get a decoy, game over (you lose)\n",
    "        if( self.pos in self.decoys ):\n",
    "          done = True\n",
    "          reward += -1.0\n",
    "        \n",
    "        if( self.inventory != old_inventory and not done ):\n",
    "          reward += 1.0\n",
    "            \n",
    "        if( self.episode_horizon != -1 and self.num_steps >= self.episode_horizon ):\n",
    "          done = True\n",
    "\n",
    "        obs = self._generate_obs()\n",
    "       \n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        random.seed() # in particular, we use this seed for the box colours\n",
    "        self.decoys = []\n",
    "        self.num_steps = 0\n",
    "        \n",
    "        self.colours = createColoursArrayHSV()\n",
    "        self.sol_length = random.randint(self.min_sol_length,self.max_sol_length)\n",
    "        self.num_decoy_paths = random.randint(0,self.max_decoy_paths)\n",
    "\n",
    "        self.board, self.decoys = boxworld.generatePuzzle(numCols=self.num_cols,\n",
    "                                             numRows=self.num_rows,\n",
    "                                             solutionLength=self.sol_length,\n",
    "                                             numDecoyPaths=self.num_decoy_paths,\n",
    "                                             maxDecoyLength=self.max_decoy_length,\n",
    "                                             multiLock=self.multi_lock,\n",
    "                                             hasBridge=self.has_bridge)\n",
    "        self.pos = [0,0] # (x,y)\n",
    "        self.inventory = []\n",
    "        \n",
    "        obs = self._generate_obs()\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def render(self, mode='rgb_array'):\n",
    "        obs = self._generate_obs()\n",
    "        \n",
    "        # Scale up for the video\n",
    "        SCALE_FACTOR = 10\n",
    "        obs_scaled = np.zeros([self.num_rows*SCALE_FACTOR, (self.num_cols+1)*SCALE_FACTOR,3],dtype=np.uint8)\n",
    "        \n",
    "        for y in range(self.num_rows):\n",
    "          for x in range(self.num_cols+1):\n",
    "            c = obs[y,x]\n",
    "            \n",
    "            for i in range(SCALE_FACTOR):\n",
    "              for j in range(SCALE_FACTOR):\n",
    "                obs_scaled[y*SCALE_FACTOR+i,x*SCALE_FACTOR+j] = c\n",
    "            \n",
    "        return obs_scaled\n",
    "\n",
    "    def _generate_obs(self):\n",
    "        \"\"\"Generate an RGB observation of the board\"\"\"\n",
    "        rgb_board = board_to_rgb(self.board,self.inventory,self.colours)\n",
    "        rgb_board[self.pos[1],self.pos[0]] = [127,127,127] # PLAYER_COLOUR\n",
    "        \n",
    "        return rgb_board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylaEOhWyhYv8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ray.rllib.models import ModelCatalog\n",
    "from agent_relational import BaseModel\n",
    "from agent_simplicial import SimplicialModel\n",
    "\n",
    "ModelCatalog.register_custom_model(\"base_model\", BaseModel)\n",
    "ModelCatalog.register_custom_model(\"simplicial_model\", SimplicialModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Relevant quotes from Zambaldi et al:\n",
    "\n",
    "- \"We used distributed A2C agents with off-policy corrections (Espeholt et al., 2018). Each agents consisted of 100 actors generating trajectories of experience, and a single learner, which learns pi and B using the actors’ experiences. The model updates were performed on GPU using mini-batches of 32 trajectories provided by the actors via a queue. The agents used an entropy cost of 0.005, discount of 0.99 and unroll length of 40 steps. Training was done using RMSprop optimiser with momentum of 0, epsilon of 0.1 and a decay term of 0.99. The learning rate was tuned, taking values between 1e−5 and 2e−4.\"\n",
    "- p.5 \"The training set-up consisted of Box-World levels with solution lengths of at least 1 and up to 4. This ensured that an untrained agent would have a small probability of reaching the goal by chance, at least on some levels.2 The number of distractor branches was randomly sampled from 0 to 4. Training was split into two variants of the task: one with distractor branches of length 1; another one with distractor branches of length 3 (see Figure 3).\" \n",
    "- Note their solution length 1 is our sol_length=1 (i.e. one locked box)\n",
    "\n",
    "For default IMPALA hyperparameters see\n",
    "- https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/impala/impala.py\n",
    "\n",
    "For an explanation of the various flags specific to Ray Tune, see\n",
    "- https://ray.readthedocs.io/en/latest/tune-usage.html\n",
    "- https://ray.readthedocs.io/en/latest/rllib-training.html\n",
    "\n",
    "Custom metrics:\n",
    "- https://ray.readthedocs.io/en/latest/rllib-training.html#callbacks-and-custom-metrics\n",
    "- https://github.com/ray-project/ray/blob/master/python/ray/rllib/evaluation/sampler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_episode_end(info):\n",
    "    env = (info[\"env\"].get_unwrapped())[0]\n",
    "    episode = info[\"episode\"]\n",
    "    \n",
    "    if( boxworld.GOAL_TILE in env.inventory ):\n",
    "      episode.custom_metrics[\"win\"] = 1\n",
    "    else:\n",
    "      episode.custom_metrics[\"win\"] = 0    \n",
    "\n",
    "    for i in range(0,5):\n",
    "        if( len(env.decoys) == i ):\n",
    "            if( boxworld.GOAL_TILE in env.inventory ):\n",
    "                episode.custom_metrics[\"win_on_decoys_\" + str(i)] = 1\n",
    "            else:\n",
    "                episode.custom_metrics[\"win_on_decoys_\" + str(i)] = 0\n",
    "\n",
    "# Capture logging output.\n",
    "LOG_STDOUT = True\n",
    "\n",
    "class flushfile():\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "    def __getattr__(self,name): \n",
    "        return object.__getattribute__(self.f, name)\n",
    "    def write(self, x):\n",
    "        self.f.write(x)\n",
    "        self.f.flush()\n",
    "    def flush(self):\n",
    "        self.f.flush()\n",
    "\n",
    "if( LOG_STDOUT ):\n",
    "  oldstdout = sys.stdout\n",
    "  sys.stdout = open(LOG_FILENAME, 'w')\n",
    "  sys.stdout = flushfile(sys.stdout)\n",
    "\n",
    "# You can try an object_store_memory cap to avoid RayOutOfMemoryError\n",
    "# on large PBT sessions (e.g. 10 workers) object_store_memory=5000000000\n",
    "ray.init(redis_address=REDIS_ADDRESS, log_to_driver=False)\n",
    "\n",
    "tune.run(\n",
    "        \"IMPALA\",\n",
    "        stop={\n",
    "            \"timesteps_total\": TIMESTEPS_TOTAL,\n",
    "        },\n",
    "        config={\n",
    "            \"env\": BoxWorld,\n",
    "            \"callbacks\": {\n",
    "                \"on_episode_end\": tune.function(on_episode_end)\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"custom_model\": AGENT,\n",
    "                \"custom_options\": {\"transformer_model_dim\": 64,\n",
    "                                   \"transformer_simplicial_model_dim\": 48,\n",
    "                                   \"transformer_num_heads\": 2, # default 2 (for 1-simplicial attention)\n",
    "                                   \"transformer_depth\": 2, # default 2\n",
    "                                   \"conv_padding\": \"valid\", # default \"valid\"\n",
    "                                   \"num_virtual_entities\": 2\n",
    "                                  }\n",
    "            },\n",
    "            \"monitor\": False,\n",
    "            \"ignore_worker_failures\": True,\n",
    "            \"sample_batch_size\": 40,\n",
    "            \"train_batch_size\": 1280,\n",
    "            \"num_workers\": NUM_WORKERS,\n",
    "            \"num_gpus\": NUM_GPUS,\n",
    "            \"log_level\": \"WARN\", # DEBUG INFO WARN ERROR, default is INFO\n",
    "            \"opt_type\": \"rmsprop\",\n",
    "            \"decay\": 0.99, # RMSprop\n",
    "            \"momentum\": 0.0, # RMSprop\n",
    "            \"epsilon\": 0.1, # RMSprop\n",
    "            \"entropy_coeff\": 0.005,\n",
    "            \"lr\": 2e-4,\n",
    "            \"env_config\": {\"num_rows\":7,\n",
    "                           \"num_cols\":9,\n",
    "                           \"min_sol_length\":1,\n",
    "                           \"max_sol_length\":3,\n",
    "                           \"max_decoy_paths\":0,\n",
    "                           \"max_decoy_length\":1,\n",
    "                           \"multi_lock\":True,\n",
    "                           \"has_bridge\":True,\n",
    "                           \"episode_horizon\":-1,# -1 for no horizon\n",
    "                          \"monitor_interval\":800}, \n",
    "        },\n",
    "        reuse_actors=True,\n",
    "        checkpoint_freq=50,\n",
    "        checkpoint_at_end=True,\n",
    "        max_failures=100,\n",
    "        resume=False,\n",
    ")\n",
    "\n",
    "if( LOG_STDOUT ):\n",
    "  sys.stdout = oldstdout\n",
    "    \n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual environment driving\n",
    "\n",
    "Execute the following cells if you want to play the environment by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Manually test the environment\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = BoxWorld(config={\"num_rows\":7,\n",
    "                           \"num_cols\":9,\n",
    "                           \"min_sol_length\":1,\n",
    "                           \"max_sol_length\":3,\n",
    "                           \"max_decoy_paths\":0,\n",
    "                           \"max_decoy_length\":1,\n",
    "                           \"multi_lock\":True,\n",
    "                           \"has_bridge\":True,\n",
    "                           \"episode_horizon\":-1,# -1 for no horizon\n",
    "                          \"monitor_interval\":200})\n",
    "for i in range(1000):\n",
    "  env.reset()\n",
    "\n",
    "obs = env.step(2)\n",
    "\n",
    "plt.imshow(obs[0]/255)\n",
    "plt.show()\n",
    "\n",
    "print(obs[1])\n",
    "print(obs[2])\n",
    "print(env.inventory)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "simptrans-v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
